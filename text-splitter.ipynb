{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "addd2b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/processed/Agentic_Design_Patterns.mmd\", \"r\") as f:\n",
    "    full_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06acfbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "rec_splitter = RecursiveCharacterTextSplitter(chunk_size=1750, chunk_overlap=250)\n",
    "rec_chunks = rec_splitter.split_text(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df5e57e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "614"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(rec_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "343e9f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"## References  \\n\\n\\nReferences1. LangChain Documentation on LCEL: https://python.langchain.com/v0.2/docs/core_modules/expression_language/2. LangGraph Documentation: https://langchain- ai.github.io/langgraph/3. Prompt Engineering Guide - Chaining Prompts: https://www.promptingguide.ai/techniques/chaining4. OpenAI API Documentation (General Prompting Concepts): https://platform.openai.com/docs/guides/gpt/prompting5. Crew AI Documentation (Tasks and Processes): https://docs.crewai.com/6. Google AI for Developers (Prompting Guides): https://cloud.google.com/discover/what- is- prompt- engineering?hl=en7. Vertex Prompt Optimizer https://cloud.google.com/vertex- ai/generative- ai/docs/learn/prompts/prompt- optimizer\\n\\n\\n\\n\\n## Chapter 2: Routing  \\n\\n\\n## Routing Pattern Overview  \\n\\n\\nWhile sequential processing via prompt chaining is a foundational technique for executing deterministic, linear workflows with language models, its applicability is limited in scenarios requiring adaptive responses. Real- world agentic systems must often arbitrate between multiple potential actions based on contingent factors, such as the state of the environment, user input, or the outcome of a preceding operation. This capacity for dynamic decision- making, which governs the flow of control to different specialized functions, tools, or sub- processes, is achieved through a mechanism known as routing.  \\n\\n\\nRouting introduces conditional logic into an agent's operational framework, enabling a shift from a fixed execution path to a model where the agent dynamically evaluates specific criteria to select from a set of possible subsequent actions. This allows for more flexible and context- aware system behavior.\""
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk = rec_chunks[50]\n",
    "chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e99f71b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "425.0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunk) / 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "32bbbb35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169472\n",
      "0\n",
      "169472\n",
      "Error: <html>\n",
      "<head><title>502 Bad Gateway</title></head>\n",
      "<body>\n",
      "<center><h1>502 Bad Gateway</h1></center>\n",
      "<hr><center>cloudflare</center>\n",
      "</body>\n",
      "</html>\n",
      "0\n",
      "169600\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from openai import RateLimitError, APIError, Timeout\n",
    "from pydantic import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "from time import sleep\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "book_name = \"Agentic Design Patterns\"\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# 1. Pydantic Model for Structured Output\n",
    "# -----------------------------------------------------------------------------\n",
    "class ChunkMetadata(BaseModel):\n",
    "    semantic_title: str = Field(description=\"A human-readable short title for the chunk\")\n",
    "    context_expansion: str = Field(description=\"Using the cached full book, generate a short context expansion for this chunk, no more than 2 sentences, that explains the broader context and patterns from the book relevant to this text.\")\n",
    "    section_header: str = Field(description=\"The section title or chapter the chunk belongs to\")\n",
    "    keywords: list[str] = Field(description=\"Important keywords describing this chunk\")\n",
    "\n",
    "MAX_RETRIES = 5\n",
    "BASE_SLEEP = 18  # seconds\n",
    "\n",
    "data_chunks = []\n",
    "\n",
    "for chunk in rec_chunks[:5]:\n",
    "    \n",
    "    user_message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": f\"Enrich the following chunk using the full book text:\\n\\n{chunk}\"\n",
    "    }\n",
    "\n",
    "    system_prompt={\n",
    "        \"role\": \"system\",\n",
    "        \"content\": f\"\"\"\n",
    "        You are a helpful RAG assistant. This is the full book '{book_name}'.\n",
    "\n",
    "        {full_text}\n",
    "    \"\"\"\n",
    "    }\n",
    "    \n",
    "    retries = 0\n",
    "    \n",
    "    while retries < MAX_RETRIES:\n",
    "        try:    \n",
    "            response = client.responses.parse(\n",
    "                model=\"gpt-5-nano\",\n",
    "                input=[\n",
    "                    system_prompt,  # cached full book\n",
    "                    user_message           # new chunk\n",
    "                ],\n",
    "                text_format=ChunkMetadata,\n",
    "            )\n",
    "\n",
    "            parsed = response.output_parsed\n",
    "            # print(parsed)\n",
    "            print(response.usage.input_tokens_details.cached_tokens)\n",
    "            chunk_metada = {}\n",
    "            chunk_metada[\"context_expansion\"] = parsed.context_expansion\n",
    "            chunk_metada[\"semantic_title\"] = parsed.semantic_title\n",
    "            chunk_metada[\"section_header\"] = parsed.section_header\n",
    "            chunk_metada[\"keywords\"] = parsed.keywords\n",
    "            \n",
    "            data_chunks.append({\n",
    "                \"chunk\": chunk,\n",
    "                \"chunk_metada\": chunk_metada\n",
    "            })\n",
    "            \n",
    "            sleep(BASE_SLEEP)\n",
    "            break\n",
    "        except (RateLimitError, APIError, TimeoutError) as e:\n",
    "            retries += 1\n",
    "            sleep_time = BASE_SLEEP * retries\n",
    "            print(f\"Error: {e}\")\n",
    "            sleep(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4045f3e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'by design. And above all, we must Inspire Trust, by being transparent about our methods and accountable for our outcomes.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c803fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The passage underscores the book's focus on governance in AI design. It foreshadows a commitment to transparency and accountability across agentic systems and patterns.\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed.context_expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2fee2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Trust and Transparency as Design Tenets'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed.semantic_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ea5f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A Thought Leader's Perspective: Power and Responsibility\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed.section_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14109451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['trust',\n",
       " 'transparency',\n",
       " 'accountability',\n",
       " 'guardrails',\n",
       " 'HITL',\n",
       " 'ethics',\n",
       " 'design patterns',\n",
       " 'agentic systems']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed.keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d2570b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.usage.input_tokens_details.cached_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "df484a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 5 enriched chunks to ./data/processed/enriched_chunks.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Ensure the directory exists\n",
    "output_dir = \"./data/processed/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(output_dir, \"enriched_chunks.json\")\n",
    "\n",
    "# Save the data_chunks list as a JSON file\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data_chunks, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Saved {len(data_chunks)} enriched chunks to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "655e7f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "614"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Ensure the directory exists\n",
    "enriched_chunks_path = \"./data/processed/enriched_chunks.json\"\n",
    "\n",
    "with open(enriched_chunks_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    enriched_chunks = json.load(f)\n",
    "    \n",
    "len(enriched_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4076eba1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
